{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Tecnologico-de-Monterrey-MNA/nlp-2023_Equipo-6/blob/main/Equipo_06_Semana_05.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xq_UEIE1t75y"
   },
   "source": [
    "# **Objetivo de la Actividad**\n",
    "\n",
    "\n",
    "> Trabajar con estos modelos pre-entrenados, generando el vocabulario a partir de tu conjunto de datos de entrenamiento.\n",
    "\n",
    "Para cada palabra de tu vocabulario, podrás sustituirlo por su correspondiente\n",
    "vector continuo. En caso de que no exista el vector para una palabra en particular, se puede eliminar dicha palabra, o bien sustituirla por el vector continuo más cercano.\n",
    "\n",
    "En esta actividad deberás aplicar esta segunda opción.\n",
    "\n",
    "-----\n",
    "\n",
    "*   Existen diversas propuestas para utilizar dichos vectores continuos como entrada para modelos de aprendizaje automático. En particular, en esta actividad cada enunciado será sustituido por el vector promedio de todos los tokens que lo forman.\n",
    "\n",
    "**Modelos:**\n",
    "\n",
    "Modelo de vectores **continuos/embebidos FastText**, es decir, el modelo desarrollado por Facebook en 2016."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3D1AhgmjvJ9C"
   },
   "source": [
    "## **Pregunta 1**\n",
    "\n",
    "Descarga los **3 archivos de Canvas**. En particular, el archivo de datos de **IMDb** ya no requiere transformarse **para obtener sus 1000 registros**. Al cargar los datos de los tres archivos deberás tener un **DataFrame de Pandas de 3000 registros**, con sus etiquetas. Los archivos los encuentras en Canvas y se llaman: **amazon5.txt, imdb5.txt, yelp5.txt.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "tZ_8l4dhwEWV",
    "outputId": "2759246b-9316-46dc-c8ee-dff6378bcd9e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Jorge\n",
      "[nltk_data]     Pedroza\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer, RegexpStemmer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, RepeatedStratifiedKFold\n",
    "\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "5JFdsxNv67X0",
    "outputId": "f871100e-ad5e-44d6-8054-0b3f14506b87"
   },
   "outputs": [],
   "source": [
    "# !wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz\n",
    "# !gunzip cc.en.300.vec.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6uGjkDFP4vtr"
   },
   "source": [
    "#**Aplicando NLTK**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "3nyPbMQ5v7Ly",
    "outputId": "a004dd72-af59-45a6-d205-b809d96b7065"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Jorge\n",
      "[nltk_data]     Pedroza\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Jorge\n",
      "[nltk_data]     Pedroza\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')        # Tokenizador que ayuda a dividr el texto en enunciados\n",
    "nltk.download('stopwords')    # Acceso a \"stopwords\" en varios idiomas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "uIq_dbSr0EmT",
    "outputId": "729679a0-ee9e-4209-a7a8-970a7772fd57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "# Lista de stopwords que se incluyen de manera predeterminada la suite de librerías de NLTK\n",
    "\n",
    "print(len(stopwords.words('english')))\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "fzJpjSXPvdRC",
    "outputId": "ca1e4533-83ab-4174-a586-b30b97a25031"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jorge Pedroza\\AppData\\Local\\Temp\\ipykernel_28132\\3513401339.py:8: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  dfi5 = pd.read_csv(url2, sep=' {3,4}', names=['review','label'], header=None, encoding='utf-8')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de registros de Amazon_5: (1000, 2)\n",
      "Total de registros de IMBD_5: (1000, 2)\n",
      "Total de registros de Yelp_5: (1000, 2)\n"
     ]
    }
   ],
   "source": [
    "#Se extraen los archivos desde repositorio público de GitHub\n",
    "url1 = 'https://raw.githubusercontent.com/Handrum/NLP_EQ_6/main/amazon5.txt'\n",
    "url2 = 'https://raw.githubusercontent.com/Handrum/NLP_EQ_6/main/imdb5.txt'\n",
    "url3 = 'https://raw.githubusercontent.com/Handrum/NLP_EQ_6/main/yelp5.txt'\n",
    "\n",
    "#Se cargan los archivos en dataframe de Pandas\n",
    "dfa5 = pd.read_csv(url1, sep='\\t', names=['review','label'], header=None, encoding='utf-8')\n",
    "dfi5 = pd.read_csv(url2, sep=' {3,4}', names=['review','label'], header=None, encoding='utf-8')\n",
    "dfy5 = pd.read_csv(url3, sep='\\t', names=['review','label'], header=None, encoding='utf-8')\n",
    "\n",
    "#Se imprime la forma de los dataframes\n",
    "print('Total de registros de Amazon_5:',dfa5.shape)\n",
    "print('Total de registros de IMBD_5:',dfi5.shape)\n",
    "print('Total de registros de Yelp_5:',dfy5.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "dOAHRDNEz6sY",
    "outputId": "957f62e9-18c8-4e76-ca40-925d4b4668fc"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A very, very, very slow-moving, aimless movie ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Not sure who was more lost - the flat characte...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Attempting artiness with black &amp; white and cle...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Very little music or anything to speak of.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The best scene in the movie was when Gerardo i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  label\n",
       "0  A very, very, very slow-moving, aimless movie ...      0\n",
       "1  Not sure who was more lost - the flat characte...      0\n",
       "2  Attempting artiness with black & white and cle...      0\n",
       "3         Very little music or anything to speak of.      0\n",
       "4  The best scene in the movie was when Gerardo i...      1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfi5.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "CQyhZt_z_rg0",
    "outputId": "75815526-8b4b-4b36-afd3-71a61db89f8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero de registros (3000, 2)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3000 entries, 0 to 2999\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   review  3000 non-null   object\n",
      " 1   label   3000 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 47.0+ KB\n"
     ]
    }
   ],
   "source": [
    "#Concatenar los 3,000 registros\n",
    "\n",
    "df = pd.concat([dfa5, dfi5, dfy5], ignore_index=True)\n",
    "print('Numero de registros',df.shape)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6A-W_SItAd_T"
   },
   "source": [
    "Al realizar la revisión del conjunto de datos se observa que el dataset de **imdb5** tiene 1.000 flotantes. Lo que requiere una limpieza especial para dicho dataset. --- **Sin embargo, se encontró que al cargar con el separador correcto, el siguiente paso no es necesario**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "kryovbUFAF2F"
   },
   "outputs": [],
   "source": [
    "#dfi5.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "V1bTBFzOBjes"
   },
   "outputs": [],
   "source": [
    "#no necesario\n",
    "#print(dfi5_c.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "1s3xzHiQB0HU"
   },
   "outputs": [],
   "source": [
    "#no necesario con la limpieza anterior\n",
    "#print(df['label'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "txUZL511EdAO"
   },
   "outputs": [],
   "source": [
    "#no necesario con la limpieza anterior\n",
    "#dfii5_l = dfi5.fillna(0)\n",
    "#dfii5_l.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Ci9f44O5GIA1"
   },
   "outputs": [],
   "source": [
    "#No necesario con la limpieza anterior\n",
    "#df = pd.concat([dfa5, dfii5_l, dfy5], ignore_index=True)\n",
    "#df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "N6dqTvNUIGqG"
   },
   "outputs": [],
   "source": [
    "X = df.review     # Serie de strings\n",
    "Y = df.label      # Serie de enteros 0s y 1s\n",
    "\n",
    "assert X.shape == (3000,)           # verificando que tenemos la dimensiones esperadas.\n",
    "assert Y.shape == (3000,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bl9HFj-u4mnk"
   },
   "source": [
    "# **Pregunta 2**\n",
    "\n",
    "Realiza de nuevo un proceso de limpieza. Aplica el preprocesamiento que consideres adecuado, sin embargo, deberás aplicar necesariamente alguna de las técnicas de lematización. Como aplicaremos modelos embebidos pre-entrenados, queremos palabras lo más cercanas a las existentes en un idioma, inglés en este caso. Aplica y justifica cualquier otro proceso de limpieza que consideres\n",
    "adecuado. Recuerda que en esta actividad se usarán vectores embebidos para un problema de clasificación, por lo que deberás tomar de acuerdo a este contexto. Justifica todas las transformaciones que se apliquen.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FyZ7I3P12JbD"
   },
   "source": [
    "**Procedimiento para quitar las negaciones del conjunto de stopwords en caso de ser necesario**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "OYEURcKY1t-I",
    "outputId": "4c57f3d2-5dda-4d37-b955-3cd3e925264c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n",
      "['no', 'nor', 'not', 'ain', 'aren', \"aren't\", 'don', \"don't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
      "179\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
      "139\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ma']\n"
     ]
    }
   ],
   "source": [
    "# Consideremos la siguiente lista de palabras asociada a negaciones en inglés:\n",
    "\n",
    "mystopwords = stopwords.words('english')\n",
    "\n",
    "negwords = [ 'no', 'nor', 'not', 'ain', 'aren', \"aren't\", 'don', \"don't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
    "\n",
    "#Se asigna las stopwords de NLTK a mystopwords\n",
    "mystopwords_without_neg = stopwords.words('english')\n",
    "\n",
    "#Se revisa si cada una de las palabras en negwords está presente en mystopwords\n",
    "for word in negwords:\n",
    "    if word in mystopwords_without_neg:\n",
    "      mystopwords_without_neg.remove(word)           # si la palabra está presente, quitarla de la lista\n",
    "\n",
    "#Se imprime la longitud y elementos de negwords para verificar resultados\n",
    "print(len(negwords))\n",
    "print(negwords)\n",
    "\n",
    "#Se imprime la longitud y elementos de los stop words de NLTK para verificar resultados\n",
    "print(len(mystopwords))\n",
    "print(mystopwords)\n",
    "\n",
    "print(len(mystopwords_without_neg))\n",
    "print(mystopwords_without_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rDyc80seLI_W"
   },
   "source": [
    "**NOTA**: Es importante tener en cuenta, que en el análisis de sentimientos debemos abarcar el vocabulario suficiente que nos permita entender la opinión del usuario y así mismo identificar aquellas opiniones que puden enriquecer y construir una interacción más amena con el cliente. Para ello, Se considera realizar una depuración de las negaciones que no agregan valor al resultado del diccionario que se requiere y que no definen a profundidad el sentimiento del cliente, estas se pueden ser reemplazadas por un vector continuo más cercano."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "4XLAEojC5Gbr"
   },
   "outputs": [],
   "source": [
    "def clean_tok(doc):\n",
    "    ##############################################################################\n",
    "    # AGREGA AQUÍ TUS LÍNEAS DE CÓDIGO - Pregunta 4:\n",
    "    \n",
    "    # Eliminación de signos de puntuación, caracteres especiales.\n",
    "    \n",
    "    doc = doc.lower()    #normalización a minúsculas\n",
    "    \n",
    "    #Sólo caracteres alfabéticos\n",
    "    puntuacion = re.sub(r'[^a-z]', ' ', doc)                  #considerar solo caracteres alfabéticos\n",
    "    puntuacion = re.sub(r'\\s{2, }', ' ', puntuacion.strip())  #eliminar todo tipo de espacios que se encuentren\n",
    "    \n",
    "    # Tokenizar\n",
    "    tokenizar = puntuacion.split()                            #tomar el resultado anterior y aplicar método de tokenización a partir del método split\n",
    "    \n",
    "    # Eliminación de Stopwords\n",
    "    \n",
    "    tokens = [token for token in tokenizar if (token not in mystopwords and len(token) >1)]  #en esta ocasión para elimianr los stopwords se toma la librería de corpus el mismo método.\n",
    "    \n",
    "    # FIN PARA AGREGAR TUS LÍNEAS DE CÓDIGO.\n",
    "    ##############################################################################\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "h3cUq1k-LBXt"
   },
   "outputs": [],
   "source": [
    "Xcleantok = [clean_tok(x) for x in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "QK2ETk5nMCOG",
    "outputId": "e3a3c5e6-cd15-4885-ef77-06ea306c6963"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>So there is no way for me to plug it in here i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Good case, Excellent value.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Great for the jawbone.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tied to charger for conversations lasting more...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The mic is great.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  label\n",
       "0  So there is no way for me to plug it in here i...      0\n",
       "1                        Good case, Excellent value.      1\n",
       "2                             Great for the jawbone.      1\n",
       "3  Tied to charger for conversations lasting more...      0\n",
       "4                                  The mic is great.      1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "lXM_3HnKLIxl",
    "outputId": "a41c7249-d159-4aed-d91c-c1f836c0b523"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['way', 'plug', 'us', 'unless', 'go', 'converter']\n",
      "['good', 'case', 'excellent', 'value']\n",
      "['great', 'jawbone']\n",
      "['tied', 'charger', 'conversations', 'lasting', 'minutes', 'major', 'problems']\n",
      "['mic', 'great']\n"
     ]
    }
   ],
   "source": [
    "for x in Xcleantok[0:5]:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cDs04xbyRyh2"
   },
   "source": [
    "# **Método Limpieza por Lematizazión**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "tSMPI-XoSELt",
    "outputId": "8505f8c5-7285-4b2f-cc83-b7158c2f4e3a"
   },
   "outputs": [],
   "source": [
    "#lista vacía para análisis visual\n",
    "palabras = []\n",
    "\n",
    "#ciclo para juntar todos los tokens previos en una lista\n",
    "for tokens1 in Xcleantok:\n",
    "\n",
    "  palabras.extend(tokens1)\n",
    "\n",
    "#Set para eliminar repeticiones\n",
    "dic = set(palabras)\n",
    "\n",
    "#Imprimir resultados para análisis visual\n",
    "#print(sorted(dic))\n",
    "\n",
    "\n",
    "#Se crea un objeto de la clase PorterStemmer\n",
    "#ps = PorterStemmer() #Se requiere usar lematización para utilizar el método de vectores embebidos\n",
    "\n",
    "#Se crea un objeto de la clase WordNetLemmatizer\n",
    "WNL = WordNetLemmatizer()\n",
    "\n",
    "#Definición de la función de limpieza adicional\n",
    "def clean_doc(doc):\n",
    "\n",
    "    #Se define lista vacía para los nuevos tokens\n",
    "    # tokens = stemmer_tokens(doc)\n",
    "    clean_tokens = []\n",
    "    \n",
    "    #Ciclo para la limpieza de los tokens\n",
    "    for token in doc:\n",
    "\n",
    "        no_dups_token = re.sub(r'([a-z])\\1{2,}', r'\\1\\1', token) \n",
    "        # Se aplica un filtro con regex para eliminar palabras donde se repita más de 2 veces seguidas la misma letra\n",
    "\n",
    "        \n",
    "        if len(no_dups_token) == 2 and no_dups_token.endswith('s'):\n",
    "            # Se aplica un filtro para las palabras como \"as\", \"is\", y \"us\", que no se les elimine la \"s\" final por la lematización\n",
    "            clean_tokens.append(no_dups_token)\n",
    "            continue\n",
    "        elif len(no_dups_token) == 3 and no_dups_token.endswith('ed'):\n",
    "            # Se aplica un filtro para las palabras como \"ted\", \"fed\", y \"ned\", que no se les eliminen las \"ed\" finales por la lematización\n",
    "            clean_tokens.append(no_dups_token)\n",
    "            continue\n",
    "        elif len(no_dups_token) == 4 and no_dups_token.endswith('ing'):\n",
    "            # Se aplica un filtro para las palabras como \"ping\", \"bing\", y \"ring\", que no se les eliminen las \"ing\" finales por la lematización\n",
    "            clean_tokens.append(no_dups_token)\n",
    "            continue\n",
    "            \n",
    "        #Se aplica stemming lo cual nos llevará las palabras a su base, incluyendo remover las terminaciones 'ing','ed','s'\n",
    "        #doc[j] = ps.stem(doc[j]) #Se aplica lematización en lugar de stemming\n",
    "    \n",
    "        #Sólo lematizar tokens mas de más de dos caracteres\n",
    "        if len(token) < 2:\n",
    "            continue\n",
    "    \n",
    "        #Se intenta lematizar verbo\n",
    "        lem_token = WNL.lemmatize(token,'v')\n",
    "        \n",
    "        #Si el token permanece sin cambios, intentar lematizar como sustantivo\n",
    "        if lem_token == no_dups_token:\n",
    "            lem_token = WNL.lemmatize(token, 'n')\n",
    "        \n",
    "        #Si el token permanece sin cambios, intentar lematizar como adjetivo\n",
    "        if lem_token == no_dups_token:\n",
    "            lem_token = WNL.lemmatize(token,'a')\n",
    "        \n",
    "        #Si el token permanece sin cambios, intentar lematizar como advervio\n",
    "        if lem_token == no_dups_token:\n",
    "            lem_token = WNL.lemmatize(token,'r')\n",
    "        \n",
    "        #Agregar el resultado a la lista de tokens límpios (clean_tokens)\n",
    "        clean_tokens.append(lem_token)\n",
    "\n",
    "    return clean_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "1pIMQaRSSMfd"
   },
   "outputs": [],
   "source": [
    "# Aplicamos el proceso de limpieza/normalización adicionales:\n",
    "\n",
    "Xclean = [clean_doc(x) for x in Xcleantok]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparamos la información antes y después de la limpieza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['way', 'plug', 'us', 'unless', 'go', 'converter'],\n",
       " ['good', 'case', 'excellent', 'value'],\n",
       " ['great', 'jawbone'],\n",
       " ['tied',\n",
       "  'charger',\n",
       "  'conversations',\n",
       "  'lasting',\n",
       "  'minutes',\n",
       "  'major',\n",
       "  'problems'],\n",
       " ['mic', 'great'],\n",
       " ['jiggle', 'plug', 'get', 'line', 'right', 'get', 'decent', 'volume'],\n",
       " ['several',\n",
       "  'dozen',\n",
       "  'several',\n",
       "  'hundred',\n",
       "  'contacts',\n",
       "  'imagine',\n",
       "  'fun',\n",
       "  'sending',\n",
       "  'one',\n",
       "  'one'],\n",
       " ['razr', 'owner', 'must'],\n",
       " ['needless', 'say', 'wasted', 'money'],\n",
       " ['waste', 'money', 'time']]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xcleantok[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "F0OxOXN1STP1",
    "outputId": "0f41787f-61a9-4cdc-d03c-43858433042c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['way', 'plug', 'us', 'unless', 'go', 'converter'],\n",
       " ['good', 'case', 'excellent', 'value'],\n",
       " ['great', 'jawbone'],\n",
       " ['tie', 'charger', 'conversation', 'last', 'minute', 'major', 'problem'],\n",
       " ['mic', 'great'],\n",
       " ['jiggle', 'plug', 'get', 'line', 'right', 'get', 'decent', 'volume'],\n",
       " ['several',\n",
       "  'dozen',\n",
       "  'several',\n",
       "  'hundred',\n",
       "  'contact',\n",
       "  'imagine',\n",
       "  'fun',\n",
       "  'send',\n",
       "  'one',\n",
       "  'one'],\n",
       " ['razr', 'owner', 'must'],\n",
       " ['needle', 'say', 'waste', 'money'],\n",
       " ['waste', 'money', 'time']]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xclean[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wDOBBjn9Wp__"
   },
   "source": [
    "# **Pregunta 3**\n",
    "\n",
    "Llamar Xclean a los comentarios procesados y Y a las etiquetas. Realicemos una partición aleatoria con los mismos porcentajes de la práctica pasada para poder comparar dichos resultados con los de esta actividad, a saber, 70%, 15% y 15%, para entrenamiento, validación y prueba, respectivamente. Verifica que obtienes 2,100 registros de entrenamiento y 450 para cada uno de validación y prueba.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['way', 'plug', 'us', 'unless', 'go', 'converter'],\n",
       " ['good', 'case', 'excellent', 'value'],\n",
       " ['great', 'jawbone'],\n",
       " ['tie', 'charger', 'conversation', 'last', 'minute', 'major', 'problem'],\n",
       " ['mic', 'great'],\n",
       " ['jiggle', 'plug', 'get', 'line', 'right', 'get', 'decent', 'volume'],\n",
       " ['several',\n",
       "  'dozen',\n",
       "  'several',\n",
       "  'hundred',\n",
       "  'contact',\n",
       "  'imagine',\n",
       "  'fun',\n",
       "  'send',\n",
       "  'one',\n",
       "  'one'],\n",
       " ['razr', 'owner', 'must'],\n",
       " ['needle', 'say', 'waste', 'money'],\n",
       " ['waste', 'money', 'time']]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xclean[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "w88pAxNzX3Zu",
    "outputId": "8a8ad41c-8dfe-4e45-b758-50157807f75d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X,y Train: 2100 2100\n",
      "X,y Val: 450 450\n",
      "X,y Test 450 450\n"
     ]
    }
   ],
   "source": [
    "# Xclean = Comentarios procesados\n",
    "# Y = etiquetas\n",
    "\n",
    "x_train, x_val_and_test, y_train, y_val_and_test = train_test_split(Xclean, Y, train_size=.70, shuffle=True, random_state=1)\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_val_and_test, y_val_and_test, test_size=.50, shuffle=True, random_state=17)\n",
    "\n",
    "print('X,y Train:', len(x_train), len(y_train))\n",
    "print('X,y Val:', len(x_val), len(y_val))\n",
    "print('X,y Test', len(x_test), len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Pregunta 4**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Pregunta 4 A**\n",
    "\n",
    "***Usa el conjunto de entrenamiento para generar tu vocabulario con un tamaño que\n",
    "consideres adecuado. Puedes filtrar tu vocabulario por la frecuencia mínima de uso de cada\n",
    "palabra, así como por su longitud mínima en caracteres.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Definir parámetros de filtrado\n",
    "frecuencia_minima = 2\n",
    "longitud_minima = 3\n",
    "\n",
    "# Contar las palabras en el conjunto de entrenamiento\n",
    "contador_palabras = Counter()\n",
    "for comentario in x_train:\n",
    "    contador_palabras.update(comentario)\n",
    "\n",
    "# Filtrar el vocabulario\n",
    "vocabulario = {palabra for palabra, frecuencia in contador_palabras.items()\n",
    "               if frecuencia >= frecuencia_minima and len(palabra) >= longitud_minima}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Pregunta 4 B**\n",
    "\n",
    "**Indica el tamaño del vocabulario generado.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del vocabulario generado: 1401\n"
     ]
    }
   ],
   "source": [
    "# Mostrar el tamaño del vocabulario generado\n",
    "print(f\"Tamaño del vocabulario generado: {len(vocabulario)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Pregunta 4 C**\n",
    "\n",
    "**¿Por qué debe usarse solamente el conjunto de entrenamiento para generar el vocabulario?**\n",
    "\n",
    "Usar solo el conjunto de entrenamiento para generar el vocabulario es importante por las siguientes razones:\n",
    "\n",
    "Evita la fuga de datos (data leakage): La fuga de datos ocurre cuando la información del conjunto de validación o prueba se usa para crear el modelo, lo que puede dar lugar a un rendimiento inflado que no se reflejará en datos nuevos.\n",
    "Representación realista: El vocabulario debe reflejar lo que el modelo encontrará en datos no vistos. Usar solo el conjunto de entrenamiento asegura que el modelo no se adapte específicamente a las características de los conjuntos de validación y prueba.\n",
    "Evaluación justa: Para evaluar de manera justa el rendimiento del modelo, los conjuntos de validación y prueba deben mantenerse independientes de cualquier proceso de modelado que involucre el conjunto de entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Pregunta 4 D**\n",
    "\n",
    "\n",
    "**Con el vocabulario generado, filtra los conjuntos de entrenamiento, validación y prueba para\n",
    "que todos los comentarios usen solamente las palabras de este vocabulario.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del conjunto de entrenamiento filtrado: 2100\n",
      "Tamaño del conjunto de validación filtrado: 450\n",
      "Tamaño del conjunto de prueba filtrado: 450\n"
     ]
    }
   ],
   "source": [
    "def filtrar_comentarios(comentarios, vocabulario):\n",
    "    return [[palabra for palabra in comentario if palabra in vocabulario] for comentario in comentarios]\n",
    "\n",
    "# Filtrar los conjuntos de datos\n",
    "x_train_filtrado = filtrar_comentarios(x_train, vocabulario)\n",
    "x_val_filtrado = filtrar_comentarios(x_val, vocabulario)\n",
    "x_test_filtrado = filtrar_comentarios(x_test, vocabulario)\n",
    "\n",
    "# Verificar el tamaño de los conjuntos filtrados\n",
    "print(f\"Tamaño del conjunto de entrenamiento filtrado: {len(x_train_filtrado)}\")\n",
    "print(f\"Tamaño del conjunto de validación filtrado: {len(x_val_filtrado)}\")\n",
    "print(f\"Tamaño del conjunto de prueba filtrado: {len(x_test_filtrado)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3_ySQygSEdcK"
   },
   "source": [
    "# **Pregunta 5**\n",
    "Utilizarás los vectores embebidos FastText preentrenados por Facebook.\n",
    "\n",
    "Incluye una tabla comparativa de pros y contras entre los modelos FastText, word2vec de Google y Glove de Stanford. Puedes consultar sus páginas correspondientes:\n",
    "- https://fasttext.cc/\n",
    "- https://code.google.com/archive/p/word2vec/\n",
    "- https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DD8-dBWo9T9j"
   },
   "source": [
    "| Características | FastText | Word2Vec | GloVe |\n",
    "| ----------- | ----------- | ----------- | ----------- |\n",
    "| **Institución desarrolladora** | Facebook AI | Google | Stanford University |\n",
    "| **Modelo** | Basado en la arquitectura de skip-gram y CBOW | Basado en las arquitecturas de skip-gram y CBOW | Basado en la matriz de co-ocurrencia de palabras en el corpus |\n",
    "| **Manejo de palabras fuera del vocabulario (OOV)** | Buen manejo gracias al uso de n-grams subwords | No lo soporta | No lo soporta |\n",
    "| **Manejo de palabras raras** | Buen manejo gracias al uso de n-grams subwords | Puede tener dificultades con palabras raras | Buen manejo dependiendo de la frecuencia en la matriz de co-ocurrencia |\n",
    "| **Velocidad de entrenamiento** | Media | Rápido | Lento en comparación con FastText y Word2Vec debido al uso de la matriz de co-ocurrencia |\n",
    "| **Dimensiones de los vectores** | Configurable, hasta 300 dimensiones | Configurable, hasta 300 dimensiones | Configurable, hasta 300 dimensiones |\n",
    "| **Requisitos de memoria** | Moderados, debido a los subwords n-grams | Bajos, sólo necesita palabras y contextos | Altos, debido a la matriz de co-ocurrencia |\n",
    "| **Interpretación semántica** | Buena, similar a Word2Vec | Buena, basada en contextos cercanos | Buena, basada en relaciones semánticas y sintácticas |\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "Es importante mencionar que cada uno de estos modelos puede ser el más adecuado dependiendo de las necesidades específicas del problema que se esté abordando.\n",
    "\n",
    "Por ejemplo, si necesitas manejar palabras fuera del vocabulario o palabras raras, FastText podría ser la mejor opción; si la memoria es una limitación, Word2Vec podría ser más adecuado; GloVe, por otro lado, podría ser más útil para capturar relaciones semánticas y sintácticas entre palabras.\n",
    "\n",
    "Por otro lado, GloVe podría desempeñarse mejor que Word2Vec para tareas de analogía de palabras, y también muestra un desempeño superior en tareas de similitud y de reconocimiento de nombre de entidades. Mientras que FastText logra un mejor desempeño en tareas sintácticas. Por último, Word2Vec sobrepasa a FastText en tareas semánticas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Bf1J3AiEt-d"
   },
   "source": [
    "# **Pregunta 6**\n",
    "Utiliza el modelo **FastText** de vectores embebidos pre-entrenados de dimensión 300 para generar un nuevo diccionario clave-valor, donde la “clave” será cada token o palabra de tu vocabulario y el “valor” será su vector embebido de dimensión 300. Este diccionario deberá ser del mismo tamaño que el vocabulario previo que hayas construido previamente.\n",
    "https://fasttext.cc/docs/en/crawl-vectors.html\n",
    "\n",
    "**NOTA**: Debido a la cantidad de recursos computacionales que demanda cargar los vectores FastText (son 2 millones de vectores), es recomendable que una vez que generes el nuevo vocabulario de vectores embebidos, guardes dicho diccionario en un archivo (pickle, npz o el que consideres más adecuado). Una vez realizado lo anterior, puedes borrar la variable de FastText para liberar memoria RAM. De esta manera, ya tienes tu vocabulario de vectores embebidos de acuerdo a los tokens que consideras más adecuados para tu problema y puedes usarlo rápidamente\n",
    "cuando lo necesites. En dado caso apóyense entre los miembros del equipo de tener dificultades para generar el vocabulario y por mientras puedes usar el archivo del vocabulario que alguno haya generado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9H3SjaXtFjbJ"
   },
   "source": [
    "**Se instala el módulo de Cython y FastText**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "W8ZJ0jF1EcUq",
    "outputId": "b4cbd8d5-442c-47f8-e892-81230480001b"
   },
   "outputs": [],
   "source": [
    "# !pip install fasttext\n",
    "# !pip install Cython --install-option=\"--no-cython-compile\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2QeNah30Fz6T"
   },
   "source": [
    "**Se importa el módulo de fasttext**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "WcwHjBoFF6sl",
    "outputId": "b031ab24-e668-45b9-caf3-e050fa51f3c5"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fasttext'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfasttext\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfasttext\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#Se descargan los el modelo de idioma inglés\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'fasttext'"
     ]
    }
   ],
   "source": [
    "import fasttext\n",
    "import fasttext.util\n",
    "\n",
    "#Se descargan los el modelo de idioma inglés\n",
    "fasttext.util.download_model('en', if_exists='ignore')  # English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "KWrcylNjJVw4",
    "outputId": "fcfa4952-a72f-4860-8c69-c3250c8949ba"
   },
   "outputs": [],
   "source": [
    "#Se utiliza el modelo FastText de vectores embebidos pre-entrenados de dimensión 300\n",
    "ft = fasttext.load_model('cc.en.300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "Cizy_S4-LDa0",
    "outputId": "6a43a925-62ad-4f33-c6cd-feb164dc738b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longitud del vocabulario de vectores embebidos: 1401\n",
      "Las dimensiones de midicc_vec son: 1401,300\n",
      "[('oven', array([ 0.07280932, -0.02370244, -0.05169982,  0.16757505,  0.177044  ,\n",
      "       -0.00364497,  0.08935895,  0.1375987 ,  0.11373347,  0.08866402,\n",
      "       -0.04330574,  0.09352942,  0.11257228, -0.09670949,  0.19062184,\n",
      "        0.06032889, -0.16845928,  0.18443851, -0.09148218,  0.00329061,\n",
      "       -0.13723697,  0.21887699, -0.16951388, -0.01393365,  0.08403366,\n",
      "       -0.05228687,  0.08536837,  0.02007805,  0.04613523,  0.38683817,\n",
      "        0.05466906,  0.2839953 , -0.07634079,  0.14579573, -0.01009578,\n",
      "        0.010638  ,  0.02913181,  0.25452062, -0.19712773, -0.1252984 ,\n",
      "        0.00705999, -0.00694526, -0.03005858, -0.02225715, -0.30444366,\n",
      "        0.01189581,  0.01983473,  0.06075285,  0.16756222, -0.07386865,\n",
      "        0.2288286 ,  0.10335812,  0.11482974, -0.01085357, -0.00372904,\n",
      "        0.01727128, -0.00283727, -0.05555332, -0.16086215, -0.01426944,\n",
      "       -0.02262048,  0.10621859, -0.12174182,  0.0364598 , -0.00499264,\n",
      "       -0.05852968, -0.00399997, -0.12883848, -0.01564604,  0.12916309,\n",
      "        0.07600918,  0.18472695, -0.01203825, -0.07867141, -0.06742756,\n",
      "        0.08116812, -0.08604839,  0.06378662, -0.0580496 , -0.04449576,\n",
      "        0.14531988, -0.04630185, -0.02050653, -0.03478435, -0.03776194,\n",
      "       -0.10923779, -0.02534892,  0.04183172,  0.01498202, -0.02701708,\n",
      "       -0.20716432, -0.05525711,  0.01924278,  0.02296206,  0.02812334,\n",
      "        0.07049742,  0.12887713,  0.0955462 ,  0.09410217, -0.0623523 ,\n",
      "       -0.11883417,  0.08606317,  0.06852647,  0.0134443 ,  0.0882827 ,\n",
      "       -0.02584135, -0.12390082, -0.09826057, -0.0954991 , -0.10061022,\n",
      "        0.13992624, -0.04233405,  0.12282099,  0.06509034,  0.01066864,\n",
      "        0.02287   ,  0.03308209,  0.12672031,  0.01687425, -0.0065997 ,\n",
      "        0.11368321,  0.0805127 , -0.0288904 ,  0.3337967 , -0.2203303 ,\n",
      "        0.05926996,  0.172208  , -0.06091423,  0.14978093,  0.00916016,\n",
      "        0.14650546,  0.00374107,  0.01558182,  0.05119326, -0.05789908,\n",
      "        0.00763166,  0.10162783,  0.1855582 , -0.09659681, -0.05504022,\n",
      "       -0.05076407, -0.15614355,  0.08901553,  0.07196774, -0.05363091,\n",
      "       -0.01492921, -0.3346358 ,  0.1005789 , -0.16686392,  0.06349569,\n",
      "        0.02919199, -0.05225266,  0.14623998, -0.03140578, -0.07866638,\n",
      "       -0.09199064,  0.10713464,  0.0072829 , -0.07355287, -0.08533918,\n",
      "        0.00677787, -0.10831323,  0.12259421,  0.04436412, -0.00120085,\n",
      "       -0.07560648, -0.23958068,  0.03750334,  0.05727304,  0.11087965,\n",
      "       -0.0929582 ,  0.01014955, -0.09664617, -0.12126629,  0.09942031,\n",
      "        0.16804126, -0.13073754, -0.08242777,  0.17344305,  0.02939462,\n",
      "       -0.19749503, -0.00147964, -0.08198696, -0.13767438,  0.05280433,\n",
      "       -0.04142048, -0.06127425,  0.07903042,  0.01985009,  0.05541619,\n",
      "        0.07355154, -0.12311788, -0.00127048, -0.14977345,  0.0788432 ,\n",
      "        0.16565824,  0.10698201, -0.11382489,  0.17753574, -0.03777003,\n",
      "        0.18645549,  0.01547579,  0.17793807, -0.18973264, -0.1599488 ,\n",
      "        0.02038468,  0.14855999, -0.11929707,  0.10194378,  0.03366474,\n",
      "        0.01736495,  0.07232025, -0.0450373 ,  0.12640661, -0.12639484,\n",
      "        0.05731134,  0.26786593,  0.02344926,  0.16785339,  0.07824409,\n",
      "       -0.00976906, -0.00370689,  0.10034834,  0.07441489, -0.02986365,\n",
      "       -0.06951915,  0.2052235 ,  0.09260666,  0.07264946, -0.0623886 ,\n",
      "       -0.07429649,  0.04919608, -0.0231862 ,  0.12867872,  0.00117778,\n",
      "        0.03291284, -0.01163366,  0.217207  , -0.00320866,  0.29761988,\n",
      "       -0.12919207,  0.02701356, -0.19727603, -0.1239357 ,  0.04976228,\n",
      "        0.22836807, -0.0391155 ,  0.21520889, -0.024959  , -0.06893972,\n",
      "        0.02059775,  0.06996339, -0.1856018 , -0.02437628,  0.00491775,\n",
      "       -0.07236059, -0.09708391, -0.1792535 ,  0.07881531,  0.22829577,\n",
      "        0.23052846, -0.0284261 ,  0.11268543,  0.09732332, -0.04516864,\n",
      "       -0.08125216,  0.09666829,  0.03349444,  0.01650624,  0.05374921,\n",
      "       -0.03490594,  0.17390236, -0.1806824 ,  0.03835861, -0.0869173 ,\n",
      "       -0.10879123, -0.05339023, -0.08767469,  0.01650154,  0.029431  ,\n",
      "        0.19904773,  0.20845568, -0.06146411, -0.03696583, -0.0073474 ,\n",
      "       -0.0981271 ,  0.02106322, -0.17062354,  0.11423144, -0.04347928,\n",
      "        0.08993556, -0.3523353 , -0.05292218, -0.19284093, -0.06817386,\n",
      "        0.00275452, -0.08003198,  0.03132779,  0.17239809, -0.19522439],\n",
      "      dtype=float32))]\n"
     ]
    }
   ],
   "source": [
    "#Copia de midicc para realizar operaciones\n",
    "midicc_vec = {}\n",
    "\n",
    "#asignar el vector resultante a cada elemento del diccionario\n",
    "for word in vocabulario:\n",
    "    vec_word = ft.get_word_vector(word)\n",
    "    midicc_vec[word] = vec_word\n",
    "\n",
    "print('Longitud del vocabulario de vectores embebidos:', len(midicc_vec))\n",
    "print('Las dimensiones de midicc_vec son: {},{}'.format(len(midicc_vec), len(list(midicc_vec.items())[0][1])))   # veamos algunos elementos del diccionario.\n",
    "print(list(midicc_vec.items())[0:1])     # veamos algunos elementos del diccionario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import fasttext\n",
    "# import fasttext.util\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diccionario de vectores embebidos guardado en 'diccionario_vectores.pkl'\n"
     ]
    }
   ],
   "source": [
    "with open('diccionario_vectores.pkl', 'wb') as archivo:\n",
    "    pickle.dump(midicc_vec, archivo)\n",
    "\n",
    "print(\"Diccionario de vectores embebidos guardado en 'diccionario_vectores.pkl'\")\n",
    "\n",
    "# Liberar memoria\n",
    "del ft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CpRFOjOvwArc"
   },
   "source": [
    "# **Pregunta 7**\n",
    "Una manera de utilizar los vectores embebidos con modelos de aprendizaje automático en\n",
    "documentos de texto, es asignar a cada comentario filtrado el vector embebido de dimensión 300\n",
    "que resulta de promediar todos sus tokens. Así, en este ejercicio deberás generar los arreglos\n",
    "correspondientes para los conjuntos de entrenamiento, validación y prueba. Los llamaremos\n",
    "trainEmb, valEmb y testEmb, respectivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones de trainEmb: (2100, 300)\n",
      "Dimensiones de valEmb: (450, 300)\n",
      "Dimensiones de testEmb: (450, 300)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# Paso 1: Cargar el diccionario de vectores embebidos\n",
    "with open('diccionario_vectores.pkl', 'rb') as archivo:\n",
    "    diccionario_vectores = pickle.load(archivo)\n",
    "\n",
    "# Paso 2: Definir la función para promediar los vectores embebidos de un comentario\n",
    "def promedio_vectores(comentario, diccionario_vectores, dimension=300):\n",
    "    vectores = [diccionario_vectores[palabra] for palabra in comentario if palabra in diccionario_vectores]\n",
    "    if len(vectores) == 0:\n",
    "        return np.zeros(dimension)\n",
    "    promedio = np.mean(vectores, axis=0)\n",
    "    return promedio\n",
    "\n",
    "# Paso 3: Generar los conjuntos de vectores embebidos promediados\n",
    "trainEmb = np.array([promedio_vectores(comentario, diccionario_vectores) for comentario in x_train_filtrado])\n",
    "valEmb = np.array([promedio_vectores(comentario, diccionario_vectores) for comentario in x_val_filtrado])\n",
    "testEmb = np.array([promedio_vectores(comentario, diccionario_vectores) for comentario in x_test_filtrado])\n",
    "\n",
    "# Verificar las dimensiones de los conjuntos generados\n",
    "print(f\"Dimensiones de trainEmb: {trainEmb.shape}\")\n",
    "print(f\"Dimensiones de valEmb: {valEmb.shape}\")\n",
    "print(f\"Dimensiones de testEmb: {testEmb.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P-unf1wrx3aA"
   },
   "source": [
    "# **Pregunta 8**\n",
    "Utiliza los modelos de regresión lineal y bosque aleatorio (random forest) y encuentra sus\n",
    "desempeños (accuracy). Compara los resultados con los de la semana anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_ByrS1Kvytfg"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, RepeatedStratifiedKFold, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Random Forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 216 candidates, totalling 648 fits\n",
      "Mejores hiperparámetros: {'bootstrap': True, 'max_depth': 10, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "Bosque Aleatorio con Hiperparámetros Óptimos - Exactitud en el conjunto de entrenamiento: 0.991904761904762\n",
      "Bosque Aleatorio con Hiperparámetros Óptimos - Exactitud en el conjunto de validación: 0.7666666666666667\n",
      "Reporte de clasificación para el conjunto de validación:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.76      0.76       216\n",
      "           1       0.78      0.77      0.77       234\n",
      "\n",
      "    accuracy                           0.77       450\n",
      "   macro avg       0.77      0.77      0.77       450\n",
      "weighted avg       0.77      0.77      0.77       450\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Definir los hiperparámetros a buscar\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "# Configurar el modelo con validación cruzada\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)\n",
    "grid_search.fit(trainEmb, y_train)\n",
    "\n",
    "# Obtener los mejores hiperparámetros\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Mejores hiperparámetros:\", best_params)\n",
    "\n",
    "# Entrenar el modelo con los mejores hiperparámetros\n",
    "best_rf = grid_search.best_estimator_\n",
    "\n",
    "# Realizar predicciones\n",
    "y_train_pred_best_rf = best_rf.predict(trainEmb)\n",
    "y_val_pred_best_rf = best_rf.predict(valEmb)\n",
    "\n",
    "# Evaluar el modelo\n",
    "accuracy_train_best_rf = accuracy_score(y_train, y_train_pred_best_rf)\n",
    "accuracy_val_best_rf = accuracy_score(y_val, y_val_pred_best_rf)\n",
    "\n",
    "print(\"Bosque Aleatorio con Hiperparámetros Óptimos - Exactitud en el conjunto de entrenamiento:\", accuracy_train_best_rf)\n",
    "print(\"Bosque Aleatorio con Hiperparámetros Óptimos - Exactitud en el conjunto de validación:\", accuracy_val_best_rf)\n",
    "print(\"Reporte de clasificación para el conjunto de validación:\")\n",
    "print(classification_report(y_val, y_val_pred_best_rf))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 15 candidates, totalling 45 fits\n",
      "Mejores hiperparámetros para regresión logística: {'C': 10, 'max_iter': 100, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Regresión Logística con Hiperparámetros Óptimos - Exactitud en el conjunto de entrenamiento: 0.8461904761904762\n",
      "Regresión Logística con Hiperparámetros Óptimos - Exactitud en el conjunto de validación: 0.7933333333333333\n",
      "Reporte de clasificación para el conjunto de validación:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.81      0.79       216\n",
      "           1       0.81      0.78      0.80       234\n",
      "\n",
      "    accuracy                           0.79       450\n",
      "   macro avg       0.79      0.79      0.79       450\n",
      "weighted avg       0.79      0.79      0.79       450\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Definir los hiperparámetros a buscar\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],\n",
    "    'penalty': ['l2'],  # 'l1' y 'elasticnet' requieren solver diferente\n",
    "    'solver': ['lbfgs'],  # Puedes usar 'liblinear' para 'l1'\n",
    "    'max_iter': [100, 200, 300]\n",
    "}\n",
    "\n",
    "# Configurar el modelo con validación cruzada\n",
    "logreg_model = LogisticRegression()\n",
    "grid_search_logreg = GridSearchCV(estimator=logreg_model, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)\n",
    "grid_search_logreg.fit(trainEmb, y_train)\n",
    "\n",
    "# Obtener los mejores hiperparámetros\n",
    "best_params_logreg = grid_search_logreg.best_params_\n",
    "print(\"Mejores hiperparámetros para regresión logística:\", best_params_logreg)\n",
    "\n",
    "# Entrenar el modelo con los mejores hiperparámetros\n",
    "best_logreg = grid_search_logreg.best_estimator_\n",
    "\n",
    "# Realizar predicciones\n",
    "y_train_pred_best_logreg = best_logreg.predict(trainEmb)\n",
    "y_val_pred_best_logreg = best_logreg.predict(valEmb)\n",
    "\n",
    "# Evaluar el modelo\n",
    "accuracy_train_best_logreg = accuracy_score(y_train, y_train_pred_best_logreg)\n",
    "accuracy_val_best_logreg = accuracy_score(y_val, y_val_pred_best_logreg)\n",
    "\n",
    "print(\"Regresión Logística con Hiperparámetros Óptimos - Exactitud en el conjunto de entrenamiento:\", accuracy_train_best_logreg)\n",
    "print(\"Regresión Logística con Hiperparámetros Óptimos - Exactitud en el conjunto de validación:\", accuracy_val_best_logreg)\n",
    "print(\"Reporte de clasificación para el conjunto de validación:\")\n",
    "print(classification_report(y_val, y_val_pred_best_logreg))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FcHKK4bIb2j-"
   },
   "source": [
    "*Se obtienen resultados similares a los obtenidos en la semana anterior. El modelo de regresión logística logra exactitudes por encima del 72% y el modelo de Bosques Aleatorios logra resultados similares y conserva su tendencia a sobreentrenarse. Para evitar lo anterios, se probaron varios métodos:*\n",
    "\n",
    "1. *Implementar validación cruzada.*\n",
    "2. *Reducir el número de estimadores.*\n",
    "\n",
    "*El primero no ayudó a reducir la tendencia a sobreentrenar el modelo. El seguno al reducirse a un valor de entr 2 y 10, logró con efectividad reducir el sobreentrenamiento. Sin embargo, la exactitud del conjunto de validación, también resultaba afectada negativamente, por lo tanto, se conservó el modelo sobreentrenado, ya que a pesar de esto, también logra los mejores resultados en el conjunto de validación. Entrenar el modelo con más datos, parece ser la mejor opción para lograr reducir el sobreentrenamiento en el modelo de bosques aleatorios.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L3jy454oblEv"
   },
   "source": [
    "# **Pregunta 9**\n",
    "Obtener la matriz de confusión e interpretar sus valores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de confusión:\n",
      "[[170  46]\n",
      " [ 44 190]]\n",
      "Reporte de clasificación:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.79      0.79       216\n",
      "           1       0.81      0.81      0.81       234\n",
      "\n",
      "    accuracy                           0.80       450\n",
      "   macro avg       0.80      0.80      0.80       450\n",
      "weighted avg       0.80      0.80      0.80       450\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Realizar predicciones en el conjunto de prueba\n",
    "y_test_pred_best_logreg = best_logreg.predict(testEmb)\n",
    "\n",
    "# Calcular la matriz de confusión\n",
    "conf_matrix = confusion_matrix(y_test, y_test_pred_best_logreg)\n",
    "\n",
    "# Calcular el reporte de clasificación\n",
    "class_report = classification_report(y_test, y_test_pred_best_logreg)\n",
    "\n",
    "# Mostrar la matriz de confusión\n",
    "print(\"Matriz de confusión:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Mostrar el reporte de clasificación\n",
    "print(\"Reporte de clasificación:\")\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QglSgTQkelAN"
   },
   "source": [
    "*Con relación a los resultados de las matrices de confusión por medio de ambos métodos, el modelo de bosques aleatorios obtuvo valor menor de clasificaciones verdaderas positivas, pero mayor en verdaderas negativas, logrando una cantidad neta de clasificaciones correctas mayor que el método de regresión logística. Se logró reducir el número de clasificaciones falsas negativas y se incrementó proporcionalmente el número falsos positivos. Al igual que lo descrito anteriormente, el valor neto clasificaciones incorrectas es menor que por el método de conteo. Lo anterior en resumen, implica que el método de bosques aleatorios es mejor para los algoritmos de clasificación, para diferenciar mejor las clases negativas en general que el método de conteo a cambio de una ligera degradación en la clasificación de clases positivas.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q4CmKx0z6NNb"
   },
   "source": [
    "# **Pregunta 10**\n",
    "Comenta con tus compañeros de equipo los pasos realizados en esta actividad e incluyan sus conclusiones finales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hGcUa5OhB_oP"
   },
   "source": [
    ">>> *Agregar aquí conclusiones del trabajo*\n",
    "\n",
    "*Dentro de la limpieza realizada se encontraron algunas validaciones que mejoraron el desempeño del modelo dado que no se procesaron ciertos valores o caracteres que no son relevantes para el estudio y alteran el resultado esperado. Posteriormente, se ejecutaron las particiones de los elementos para el desarrollo de los modelos; para ello, se acotaron los caracteres dentro del diccionario del conjunto de entrenamiento dado que se involucraron únicamente palabras relevantes las cuales tienen una mayor cantidad de frecuencia dentro de los datos.*\n",
    "\n",
    "*Luego del preprocesamiento anterior, se inició la validación del modelo **FastText** con el fin de identificar la relación que tienen las palabras que contiene el diccionario definido. Luego de inicializarlo, se desarrollaron las funciones requeridas para obtener el promedio de los vectores y de allí otra función que transforme los documentos a vectores embebidos con el fin de asignarles valores a los vectores e identificar la correlación de cada uno entre ellos.*\n",
    "\n",
    "*Con los procedimientos anteriores, se utilizaron dichos vectores para optimizar los modelos de Random Forest y Linear Regression, obteniendo su desempeño y la validación de estos. Detectando los hiperparámetros requeridos para cada modelo y obteniendo los resultados del desempeño de cada uno. **FastText** permite una mejor predicción dado que asigna una mejor relación entre las palabras que únicamente valores de 1 y 0 para los caracteres obtenidos, distorsionando el valor del desempeño de los modelos.*\n",
    "\n",
    "*Para este caso, el método de transformar el corpus a sus vectores preentrenados embebidos, después de el proceso de limpieza y lematización, para después calcular el promedio de los vectores para cada comentario, resultó en una desempeño menor que el que se obtuvo para el mismo conjunto de datos y el uso de la matriz dispersa por conteo de frecuencias y de tf-idf. Con lo anterior, se pudo comprender el valor de tener cada palabra representada como un elemento de un espacio vectorial para encontrar la relación entre las palabras cercanas, sin embargo, para este caso, el uso del promedio de dichos vectores no resultó superior en exactitud a los resultados logrados con las matrices dispersas. Posiblemente, los resultados se pueden mejorar si emplea una simplificación del comentario diferente al promedio de los vectores, como lo sería un proceso de reducción tomando en cuenta la relación que tiene una palabra con otra.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fK_RJaCNBEoj"
   },
   "source": [
    "##### **Fuentes bibliográficas y de datos:**\n",
    "- https://raw.githubusercontent.com/Handrum/NLP_EQ_6/main/amazon5.txt\n",
    "- https://raw.githubusercontent.com/Handrum/NLP_EQ_6/main/imdb5.txt\n",
    "- https://raw.githubusercontent.com/Handrum/NLP_EQ_6/main/yelp5.txt\n",
    "- https://fasttext.cc/\n",
    "- https://fasttext.cc/docs/en/crawl-vectors.html\n",
    "- https://code.google.com/archive/p/word2vec/\n",
    "- https://nlp.stanford.edu/projects/glove/\n",
    "- https://medium.com/analytics-vidhya/word2vec-glove-fasttext-and-baseline-word-embeddings-step-by-step-d0489c15d10b\n",
    "\n",
    "- Vajjala, S., Majumder, B., Gupta, A., y Surana, H. (2020). Practical Natural Language Processing: A Comprehensive Guide to Building Real-World NLP Systems. O'Reilly. https://learning.oreilly.com/library/view/practical-natural-language/9781492054047/\n",
    "\n",
    "\n",
    "- Khurana, D., Koli, A., Khatter, K., y Singh, S. (2023). Natural language processing: state of the art, current trends and challenges. Multimed Tools Appl 82, 3713–3744. https://link.springer.com/article/10.1007/s11042-022-13428-4Links to an external site.\n",
    "\n",
    "- Falcón Morales, L. E. (2023). Bolsa de palabras: BOW [PDF]. Maestría en Inteligencia Artificial Aplicada. ITESM. Acceso al material Download Acceso al material"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
